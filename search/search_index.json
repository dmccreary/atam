{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Architecture Tradeoff Analysis Methodology","text":"<p>Welcome to our ATAM website!</p> <p>This website stores resources for teaching courses on the Architecture Tradeoff Analysis Methodology (ATAM) with NoSQL databases. ATAM was originally developed by Carnegie Mellon University (CMU) for selecting the appropriate architectures for large computer projects.</p> <p>We have adapted ATAM for use in helping organizations find the best database architecture for a business challenge.</p> <p>This site supports our main textbook:</p> <p>Making Sense of NoSQL</p> <p>Please let me know if you have any questions.</p> <p>Dan McCreary on LinkedIn</p>"},{"location":"about/","title":"About the ATAM/NoSQL Project","text":"<p>Early in my career, I worked for Bell Labs. I met a coworker named Bjarne Stroustrup who was the principal driver behind C++. I was also exposed to Objective C when I worked for Steve Jobs at NeXT Computer.  I realized that it really didn't matter what flavor of programming language you used as long as you used good Design Patterns.  This brought me to read about the origin of design patterns and the incredible book A Timeless Way of Building.  The key word here was timeless because it focused on understanding the underlying principles around a topic that didn't vary with the release of the next version of a software system.</p> <p>I resolved to not write a book on the latest bugs in Windows 3.1.  The shelf life of that book was less than six months.  But if I could find the underlying patterns in the way that we represent data, that book would be timeless!</p>"},{"location":"about/#the-day-i-learned-about-exist-db","title":"The Day I Learned about eXist-DB","text":"<p>I had a transformative experience starting around Feb 2, 2007, at 11:40\u202fAM.  That was the exact day and time that my friend Kurt Cagle suggested that I try out the eXist database.  My life was about to take a surprising turn.</p> <p>I was working on a complex forms project where each form \"Save\" had to perform about 45 inserts into a RDBMS.  That process, of shredding the document into individual parts, was very complex and we had allocated six months to that project.</p> <p>But with eXist, we could perform all that work in a single line of code.  I recall staring at that single line for what must have been 20 minutes.  Could it really be this simple?  Why had I never heard about this?  Why did they only teach me about RDBMS systems in college?  How could I have been such a fool?</p> <p>I vowed NEVER to be caught off guard again.  I vowed I would travel the world to know the best way to store knowledge.  That was the basis of a new conference (NoSQL Now!) I helped orchestrate. From that came the book Making Sense of NoSQL which I wrote with my wife, Ann Kelly.</p> <p>But my journey didn't end with the publication of the book.  Granted, I did become an expert at helping companies select databases.  However, to my utter frustration, despite objective evidence, companies often made poor decisions. So I had to start to study why their cognitive bias got in the way.</p>"},{"location":"atam-db-process/","title":"ATAM Database Selection Process","text":"<p>In this section, we take the Standard ATAM Process and modify it for the specialized task of selecting the right database architecture for a project.</p> <p>Developed by the Software Engineering Institute (SEI) at Carnegie Mellon University (CMU), ATAM is a risk-mitigation process used in the early stages of software development. Its purpose is to evaluate software architectures and identify potential risks related to achieving quality attribute goals and business objectives. This method helps organizations make informed architectural decisions by uncovering tradeoffs and sensitivity points within the design. </p>"},{"location":"atam-process/","title":"Architectural Tradeoff Analysis Method Process","text":"<p>The Architectural Tradeoff Analysis Method (ATAM) is a structured method for evaluating software architecture with respect to multiple quality attributes. Developed by the Software Engineering Institute (SEI) at Carnegie Mellon University, ATAM helps to understand a system's behavior and determine if it has the right architecture to support the goals of the project. Below is a summary of the ATAM process according to the provided image.</p> <p>Goals of the ATAM Process: The primary goals of the ATAM are to:</p> <ol> <li>Identify the consequences of architectural decisions.</li> <li>Identify risks and non-risks associated with the architecture.</li> <li>Assess the tradeoffs in the architectural approach, particularly those involving quality attributes.</li> <li>Provide a framework to make informed decisions that balance these tradeoffs.</li> </ol> <p>Steps of the ATAM Process:</p> <ol> <li> <p>Business Drivers: This step involves understanding the strategic goals and objectives of the business, which will drive the architectural decisions. This includes constraints, functional and non-functional requirements, and aspirations that shape the architecture.</p> </li> <li> <p>Architecture Plan: The architecture plan is the outline or blueprint of the system's architecture. It includes details on the architectural style, patterns, and structural organization.</p> </li> <li> <p>Quality Attributes: Quality attributes are the non-functional requirements such as performance, security, maintainability, and usability that the system must satisfy.</p> </li> <li> <p>Architectural Approaches: These are the strategies or techniques used to address the quality attributes in the architecture plan, including the use of specific design patterns or systems principles.</p> </li> <li> <p>User Stories: User stories provide a high-level description of functionality from an end-user perspective. They help to ensure that the architecture addresses real user needs.</p> </li> <li> <p>Architectural Decisions: This step involves making concrete decisions about the architecture, which are informed by the business drivers, quality attributes, and architectural approaches.</p> </li> <li> <p>Analysis: The analysis is the core of the ATAM, where the architectural decisions are scrutinized. This stage is where the architectural strategies are evaluated against the desired quality attributes through various analysis techniques.</p> </li> </ol> <p>Documents Produced by the ATAM Process:</p> <ol> <li> <p>Tradeoffs: This document captures the analysis of various architectural decisions and their impact on different quality attributes, revealing where compromises are made.</p> </li> <li> <p>Sensitivity Points: These are the points in the architecture that are sensitive to changes. Understanding these helps in predicting the impact of changes on the system's quality attributes.</p> </li> <li> <p>Non-Risks: These are architectural aspects that have been determined not to pose a risk to the project. They are typically well-understood areas with known solutions.</p> </li> <li> <p>Risks: These are potential problems that could threaten the project's success. They might stem from ambitious quality attribute goals, reliance on novel technology, or other uncertainties in the architecture.</p> </li> </ol> <p>Distilled Information: As a result of the ATAM process, distilled information is produced, which encompasses the identified risks, non-risks, sensitivity points, and tradeoffs. This distilled information helps stakeholders make informed decisions about the architecture and the project.</p> <p>Risk Themes: Throughout the ATAM process, certain themes of risk may emerge. These are broad areas of concern that need to be addressed by the project team to ensure the architecture can meet its goals. Risk themes help prioritize subsequent actions and refine the architecture.</p> <p>Impacts: The identified risks, non-risks, sensitivity points, and tradeoffs have direct impacts on the project. The analysis of these impacts is crucial for planning, mitigation, and ensuring that the architecture aligns with the business drivers.</p> <p>In conclusion, the ATAM process is a comprehensive method that assesses software architecture rigorously to ensure it aligns with business goals and adequately addresses quality requirements. The process involves a detailed evaluation of tradeoffs, risks, and non-risks, culminating in a well-informed architectural strategy that is essential for the successful delivery of the software system.</p>"},{"location":"bias/","title":"Cognitive Bias in Database Selection","text":"<p>After attending the Saturn 2013 Conference I was exposed to the use of \"Cognitive Bias\" in software architecture.</p> <p>Here are some examples of cognitive bias I have seen as applied to the world of NoSQL database selection.</p>"},{"location":"bias/#anchoring-bias","title":"Anchoring Bias","text":"<p>The tendency to produce an estimate near a cue amount.</p> <p>Example: \"Our managers were expecting an RDBMS solution so that\u2019s what we gave them.\"</p>"},{"location":"bias/#availability-heuristic","title":"Availability Heuristic","text":"<p>The tendency to estimate that what is easily remembered is more likely than that which is not.</p> <p>Example: \"I hear that NoSQL does not support ACID.\" or \"I hear that XML is verbose.\"</p>"},{"location":"bias/#bandwagon-effect","title":"Bandwagon Effect","text":"<p>The tendency to do or believe what others do or believe.</p> <p>Example: \"Everyone else at this company and in our local area uses RDBMSs.\"</p>"},{"location":"bias/#confirmation-bias","title":"Confirmation Bias","text":"<p>The tendency to seek out only that information that supports one's preconceptions.</p> <p>Example: \"We only read posts from the Oracle|Microsoft|IBM groups.\"</p>"},{"location":"bias/#framing-effect","title":"Framing Effect","text":"<p>The tendency to react to how information is framed, beyond its factual content.</p> <p>Example: \"We know of some NoSQL projects that failed.\"</p>"},{"location":"bias/#gamblers-fallacy","title":"Gambler's fallacy","text":"<p>The failure to reset one's expectations based on one's current situation.</p> <p>Example: \"We already paid for our Oracle|Microsoft|IBM license so why spend more money?\"</p> <ul> <li>Also known as: sunk cost bias</li> </ul>"},{"location":"bias/#hindsight-bias","title":"Hindsight Bias","text":"<p>The tendency to assess one's previous decisions as more efficacious than they were.</p> <p>Example \"Our last five systems worked on RDBMS solutions. Why should we change now?\"</p>"},{"location":"bias/#halo-effect","title":"Halo Effect","text":"<p>The tendency to attribute unverified capabilities in a person based on an observed capability.</p> <p>Example: \"Oracle|Microsoft|IBM sells billions of dollars of licenses each year, how could so many people be wrong\". </p>"},{"location":"bias/#representativeness-heuristic","title":"Representativeness Heuristic","text":"<p>The tendency to judge something as belonging to a class based on a few salient characteristics  - \"Our accounting systems work on RDBMS so why not our product search?\"</p>"},{"location":"bias/#references","title":"References","text":"<ol> <li> <p>Cognitive Bias in Database Selection - my 2013 blog on this topic after attending the CMU SEI meeting with the wonderful Mary Poppendieck</p> </li> <li> <p>Cognitive Bias in Architectural Decisions</p> </li> </ol>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"slides/","title":"ATAM Presentations","text":"<p>You can find our presentations on the topic of ATAM and NoSQL here:</p> <p>https://github.com/dmccreary/atam/tree/main/slides.</p> <p>The license for all these slides is Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED).  We always appreciate attribution!</p>"},{"location":"case-studies/amazon-shopping-cart/","title":"Amazon Shopping Cart Failures","text":"<p>The story of Amazon's struggles with their Oracle-based back-end system during the early 2000s, particularly around the busy holiday shopping season, is a significant one in the history of web-scale computing and database management. It highlights the challenges faced by rapidly growing online businesses and the innovative solutions they developed in response.</p>"},{"location":"case-studies/amazon-shopping-cart/#the-problem","title":"The Problem","text":"<p>Around 2002, Amazon, which was rapidly growing, faced significant challenges with its Oracle relational database management system (RDBMS). The primary issues were related to scalability, reliability, and performance, especially during peak times like the holiday shopping season. The Oracle RDBMS, while robust and powerful, was not ideally suited to handle the massive, unpredictable spikes in traffic and transactions that Amazon experienced. This led to:</p> <ol> <li>Slowdowns and Outages: During peak traffic periods, the database struggled to keep up, resulting in slowdowns and outages.</li> <li>Complexity and Cost: Scaling up the Oracle database to meet the demand was complex and expensive.</li> <li>Rigid Schema: The relational model, with its rigid schema, was not flexible enough for Amazon's rapidly evolving and diverse data needs.</li> </ol>"},{"location":"case-studies/amazon-shopping-cart/#amazons-response","title":"Amazon's Response","text":"<p>Faced with these challenges, Amazon began to explore alternatives. Their response involved a fundamental shift in how they managed their data:</p> <ol> <li>Distributed Systems: Amazon moved away from a centralized RDBMS architecture to a distributed system. This approach allowed them to distribute the load across multiple nodes, improving performance and reliability.</li> <li>Microservices Architecture: They adopted a microservices architecture, breaking down their monolithic application into smaller, independent services. Each service could use the most appropriate data storage solution for its needs.</li> <li>Custom Solutions: Amazon started to build its own data storage solutions tailored to their specific requirements.</li> </ol>"},{"location":"case-studies/amazon-shopping-cart/#amazon-dynamodb","title":"Amazon DynamoDB","text":"<p>One of the most significant outcomes of Amazon's efforts to overcome the limitations of traditional RDBMS systems was the creation of Amazon DynamoDB. DynamoDB, introduced in 2012, is a fully managed NoSQL database service provided by Amazon Web Services (AWS). It was designed to address many of the issues that Amazon faced with their Oracle system:</p> <ol> <li>Scalability: DynamoDB can handle large amounts of traffic and data, scaling up or down as needed.</li> <li>Performance: It offers fast and predictable performance, even under massive load.</li> <li>Flexibility: Being a NoSQL database, it allows for more flexible data models, which is suitable for various types of applications and services.</li> <li>Reliability and Availability: DynamoDB provides high availability and durability, storing data across multiple AWS regions and Availability Zones.</li> </ol> <p>In summary, Amazon's move from an Oracle RDBMS to building and eventually offering DynamoDB as a product was a response to the scalability and flexibility challenges they faced. It represents a broader trend in the industry towards distributed, NoSQL databases for web-scale applications.</p>"},{"location":"case-studies/star-process/","title":"The S.T.A.R Process in Writing ATAM Case Studies","text":"<p>We have written hundreds of case studies about how organzations use the ATAM process.  We use the S.T.A.R approach which stands for:</p> <ol> <li>Situation - give context to the case study</li> <li>Task - what was the challenge being addressed</li> <li>Approach - What was the architectural approach?</li> <li>Results - What were the results, both measurable and intangible</li> </ol> <p>Here are some details for each of these sections:</p>"},{"location":"case-studies/star-process/#situation-context","title":"Situation (Context)","text":"<p>This is the backstory where we set the scene and provide the necessary background information. It involves describing the context within which the events or challenges occurred. For a case study, this would include details about the organization, the environment, specific circumstances, or any other relevant information that gives a clear picture of the scenario.</p>"},{"location":"case-studies/star-process/#task-challenge","title":"Task (Challenge)","text":"<p>Next, we outline the specific challenge or problem that needs to be addressed. It's about what needed to be done and why it was important. In a case study, this could involve a key pain point or the goal that the organization was trying to achieve, or a particular obstacle that needed to be overcome.</p>"},{"location":"case-studies/star-process/#architectural-approach","title":"Architectural Approach","text":"<p>In this section, I describe my architectural approach to building a solution. I briefly describe the options on the table and the tradeoff process that I used to select a specific architectural solution. This process details the strategies, processes, or steps taken to tackle the problem. The focus should be on specific actions and why those actions were chosen.</p>"},{"location":"case-studies/star-process/#results","title":"Results","text":"<p>Finally, we present the outcomes of the approach we took. This is where I showcase the results, achievements, and learnings obtained from the experience. It's important to be as quantifiable as possible, using data and specific examples to illustrate the impact of the actions. I focus on easy-to-measure dollar savings and how the organization is repositioned to be more agile in the future.</p> <p>We like this method because it provides a clear and logical structure, ensuring that all essential elements of a story or case are covered. It's particularly effective in making complex information more digestible and compelling, leading the audience through a logical progression from problem to solution.</p>"},{"location":"concepts/","title":"ATAM Database Concepts","text":""},{"location":"concepts/#cross-cutting-concerns","title":"Cross-cutting Concerns","text":"<ul> <li>Performance and Scalability: We will discuss how each type scales and performs under different workloads.</li> <li>Data Integrity and Consistency: We will contrast the approaches to data integrity and consistency, especially compared to ACID properties in RDBMS.</li> <li>Maintenance and Operational Complexity: We will evaluate the maintenance needs and operational complexity of each type.</li> <li>Security: We cover security features and concerns relevant to each database type.  We put a focus on scalable RBAC systems.</li> <li>Community and Ecosystem: We will also assess the community support, availability of tools, and integration capabilities.</li> <li>Cost Considerations: Next, we discuss cost implications, including open-source versus proprietary solutions and cloud-hosted versus on-premise.</li> <li>Trends and Future Directions: Finally, we iscuss emerging trends in database technologies and potential future developments.</li> </ul>"},{"location":"concepts/#distributed-database-concerns","title":"Distributed Database Concerns","text":"<p>Covering core architectural concepts in distributed databases is essential for understanding their capabilities, challenges, and best use cases. Here are some key concepts you should consider including in your book:</p>"},{"location":"concepts/#distributed-transactions","title":"Distributed Transactions","text":"<ul> <li>ACID Properties in Distributed Context: Explain how Atomicity, Consistency, Isolation, and Durability are maintained across multiple nodes.</li> <li>Two-Phase Commit (2PC): Discuss the two-phase commit protocol as a method of ensuring all-or-nothing transaction execution across distributed systems.</li> <li>Challenges and Trade-offs: Cover challenges like network latency, partition tolerance, and the CAP theorem's implications on distributed transactions.</li> </ul>"},{"location":"concepts/#replication","title":"Replication","text":"<ul> <li>Types of Replication: Describe synchronous and asynchronous replication, their use cases, and trade-offs.</li> <li>Consistency Models: Explain strong versus eventual consistency and their impact on data integrity and system performance.</li> <li>Conflict Resolution: Discuss how conflicts are resolved in multi-master replication scenarios.</li> <li>Replication Topologies: Cover different replication topologies like master-slave, peer-to-peer, and their implications on system resilience and read/write performance.</li> </ul>"},{"location":"concepts/#auto-sharding-data-partitioning","title":"Auto-Sharding (Data Partitioning)","text":"<ul> <li>Concept and Benefits: Explain how auto-sharding distributes data across multiple nodes to balance load and improve performance.</li> <li>Shard Key Selection: Discuss the importance of choosing the right shard key for optimal data distribution and access patterns.</li> <li>Rebalancing and Resharding: Cover the process of redistributing data when adding or removing nodes and its impact on system performance.</li> <li>Challenges: Highlight potential challenges such as hotspots and cross-shard transactions.</li> </ul>"},{"location":"concepts/#high-availability","title":"High Availability","text":"<ul> <li>Redundancy and Failover: Describe how distributed databases achieve high availability through redundancy and automated failover mechanisms.</li> <li>Load Balancing: Explain load balancing strategies for evenly distributing requests and optimizing resource utilization.</li> <li>Disaster Recovery: Discuss strategies for backup and recovery in distributed environments, including geographical distribution for disaster resilience.</li> <li>Monitoring and Health Checks: Cover the importance of monitoring system health and performing regular checks to ensure high availability.</li> </ul>"},{"location":"concepts/#cross-cutting-concepts","title":"Cross-Cutting Concepts","text":"<ul> <li>CAP Theorem: Discuss the CAP Theorem (Consistency, Availability, Partition Tolerance) and its implications for distributed database design.</li> <li>Network Partitioning and Latency: Explain the impact of network issues on distributed databases and strategies to mitigate these effects.</li> <li>Data Consistency Levels: Differentiate between various levels of data consistency (like read-your-writes, monotonic reads, etc.) in distributed systems.</li> <li>Security Considerations: Highlight security challenges unique to distributed databases, including data encryption and secure communication across nodes.</li> </ul>"},{"location":"concepts/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Global Databases and Multi-Region Deployment: Discuss the architecture and considerations for deploying globally distributed databases.</li> <li>Data Versioning and Time Travel Queries: Introduce concepts like data versioning and the ability to query data as it existed at a specific point in time.</li> <li>Observability and Debugging: Address the complexity of monitoring and debugging in distributed environments, emphasizing distributed tracing and log aggregation.</li> </ul> <p>By covering these topics, you'll provide a thorough understanding of the architectural complexities of distributed databases. These concepts are crucial for anyone looking to design, implement, or manage a distributed database system effectively.</p>"},{"location":"concepts/acid-vs-base/","title":"ACID vs. BASE","text":"<p>In this section, we will discuss the concept of ACID versus BASE in the context of distributed database systems. This contrast highlights two fundamentally different approaches to handling data consistency and availability in distributed environments.</p>"},{"location":"concepts/acid-vs-base/#acid-explained","title":"ACID Explained","text":"<p>ACID stands for Atomicity, Consistency, Isolation, and Durability. It is a set of principles aimed at ensuring reliable transaction processing in database systems.</p> <ol> <li>Atomicity: Guarantees that all operations within a transaction are treated as a single unit. Either all operations are executed successfully, or none are.</li> <li>Consistency: Ensures that a transaction brings the database from one valid state to another, maintaining all predefined rules, including constraints, cascades, and triggers.</li> <li>Isolation: Ensures that concurrently executed transactions do not affect each other. Each transaction is isolated from others until it's completed.</li> <li>Durability: Once a transaction is committed, it will remain so, even in the event of system failures. This usually involves writing to non-volatile memory or logs.</li> </ol> <p>ACID in Real-World Systems: Traditional relational databases like PostgreSQL, MySQL, and Oracle are prime examples of systems implementing ACID properties. They are used in scenarios where data integrity and consistency are non-negotiable, such as financial systems, inventory management, and any system where it's critical to prevent data anomalies.</p>"},{"location":"concepts/acid-vs-base/#base-explained","title":"BASE Explained","text":"<p>BASE stands for Basically Available, Soft state, and Eventual consistency. It's an alternative model designed for distributed systems, focusing on high availability and fault tolerance, at the cost of strong consistency.</p> <ol> <li>Basically Available: Indicates that the system guarantees availability in terms of the CAP theorem, but allows for some level of data inconsistency.</li> <li>Soft state: The state of the system may change over time, even without input. This is due to eventual consistency models where data is not immediately consistent across all nodes.</li> <li>Eventual Consistency: The system will eventually become consistent once it stops receiving input. Data replication to achieve consistency can be delayed for better performance and availability.</li> </ol> <p>BASE in Real-World Systems: NoSQL databases like Cassandra, Couchbase, and DynamoDB use the BASE model. They are suitable for applications that can tolerate some degree of inconsistency or where the emphasis is on horizontal scalability and speed, such as social networks, big data analytics, and content distribution networks.</p>"},{"location":"concepts/acid-vs-base/#contrasting-acid-and-base","title":"Contrasting ACID and BASE","text":"<ol> <li>Consistency vs. Availability: ACID prioritizes consistency (every read receives the most recent write) but may sacrifice availability (the system might not always be able to process transactions). BASE, on the other hand, prioritizes availability with the trade-off that data may not always be consistent immediately.</li> <li>System Design: Systems implementing ACID are often more straightforward to reason about but can be challenging to scale horizontally. BASE systems are designed for scale, but they require more complex designs to handle data inconsistency.</li> <li>Use Cases: ACID is essential where consistency is critical, like banking systems. BASE is preferred where scalability and handling high volumes of data with variable consistency is acceptable, like in social media feeds.</li> <li>Network Partitions: In the event of network partitions, ACID systems might stop processing transactions to maintain consistency, while BASE systems will continue to operate, accepting that the data will be inconsistent until the partition resolves.</li> </ol> <p>In summary, the choice between ACID and BASE models in distributed databases depends on the specific requirements of the application, particularly in terms of consistency needs and scalability. Understanding the trade-offs between these models is crucial for designing systems that meet the necessary reliability, availability, and performance criteria.</p>"},{"location":"concepts/four-vs-of-nosql/","title":"The Four \u201cV\u201ds of NoSQL for Scalable-Database Selection","text":"<p>When we dscuss selecting the right database, scalability is often a primary concern for any organization that needs to scale it customer base.</p> <p>There are four dimensions of scalability we will review in this chapter:</p> <ol> <li>Volume - how much data needs to be queries</li> <li>Velocity - how fast the data comes in or querys need to respond</li> <li>Variability - how much variablity is there in the data types we need to represent</li> <li>Veracity - how can we apply rules to test for data quality</li> </ol>"},{"location":"concepts/four-vs-of-nosql/#the-four-vs-of-nosql-for-scalable-database-selection_1","title":"The Four \"V\"s of NoSQL for Scalable Database Selection","text":"<p>When selecting a NoSQL database for scalable applications, understanding the four fundamental \"V\"s is crucial for making informed architectural decisions. These four dimensions\u2014Volume, Velocity, Variability, and Veracity\u2014represent the core challenges that NoSQL databases were designed to address, each presenting unique performance considerations and trade-offs.</p> <p>The emergence of NoSQL databases was largely driven by the limitations of traditional relational databases in handling these four dimensions simultaneously. While SQL databases excel in structured environments with predictable workloads, NoSQL databases offer specialized solutions for scenarios where one or more of these \"V\"s become critical bottlenecks.</p> <p>Selecting a NoSQL engine is rarely about ticking feature boxes; it is about balancing qualities that matter most to your system's stakeholders. CMU's Architecture Trade-off Analysis Method (ATAM) provides the discipline for surfacing quality-attribute scenarios and comparing design options. Within data-intensive systems, these four related attributes\u2014Volume, Velocity, Variability, and Veracity\u2014reliably dominate those scenarios.</p> <p>[Suggested Image: A diagram showing the four V's as interconnected dimensions of a cube, with NoSQL database types positioned within this space]</p>"},{"location":"concepts/four-vs-of-nosql/#volume-managing-massive-data-scale","title":"Volume: Managing Massive Data Scale","text":"<p>Volume refers to the sheer quantity of data that a system must store, process, and manage. In the NoSQL context, volume challenges extend beyond simple storage capacity to encompass distributed storage architecture, data partitioning strategies, and horizontal scaling capabilities.</p> <p>Large, ever-growing data sets stress storage layout, compaction, backup windows, and even the physics of network transfers. Traditional relational databases typically scale vertically, requiring increasingly powerful hardware to handle growing data volumes. NoSQL databases, however, are designed for horizontal scaling, distributing data across multiple nodes to handle petabyte-scale datasets efficiently.</p>"},{"location":"concepts/four-vs-of-nosql/#design-considerations-for-volume","title":"Design Considerations for Volume","text":"Design Considerations Typical NoSQL Levers Horizontal partitioning strategy Hash vs. range sharding, virtual nodes Replica-set sizing for HA vs. cost Consistency level (e.g., QUORUM vs. ONE) Compaction &amp; repair overhead Leveled/size-tiered compaction, anti-entropy Data-placement awareness Rack-/AZ-aware replica placement <p>The volume challenge manifests in several critical areas:</p> <p>Storage Architecture: NoSQL databases must efficiently distribute data across multiple physical nodes while maintaining performance and availability. This involves sophisticated partitioning strategies, replica management, and data locality optimization.</p> <p>Query Performance: As data volume grows, maintaining sub-second query response times becomes increasingly challenging. NoSQL databases employ various strategies such as denormalization, distributed indexing, and caching layers to address this challenge.</p> <p>Data Movement: Large volumes of data create significant challenges for backup, replication, and migration operations. NoSQL systems must handle these operations without impacting production performance.</p>"},{"location":"concepts/four-vs-of-nosql/#real-world-volume-challenges","title":"Real-World Volume Challenges","text":"<p>Facebook Messenger: Facebook Messenger stores petabytes of message history in Apache Cassandra, adding billions of new rows per day. Teams must tune compaction and anti-entropy jobs so that weekly repairs finish before the next cycle begins, or read latencies spike. The challenge lies not just in storing this volume of data, but in maintaining millisecond-level response times while managing the operational complexity of distributed repairs.</p> <p>Netflix's Global Scale: Netflix stores detailed viewing histories for over 200 million subscribers, generating terabytes of data daily. Their Cassandra clusters handle over 1 trillion reads and writes per day across globally distributed data centers. The volume challenge involves enabling fast search and filtering across multiple dimensions while maintaining data consistency across different regional catalogs.</p> <p>IoT Sensor Networks: Industrial IoT implementations often involve thousands of sensors generating data points every second. A smart city implementation might collect data from traffic sensors, environmental monitors, and infrastructure systems, resulting in billions of data points per day. Traditional databases struggle with this write-heavy workload, while NoSQL solutions like InfluxDB or Cassandra can handle the high-volume ingestion while maintaining queryability for analytics.</p> <p>Amazon's Product Ecosystem: Amazon's product catalog contains hundreds of millions of products, each with complex attribute sets, pricing history, and customer reviews. The volume challenge involves not just storing this data, but performing complex graph traversals across this massive dataset for features like product recommendations and cross-selling algorithms.</p> <p>[Suggested Image: A layered diagram showing logical data partitions mapped across multiple racks/AZs, with arrows illustrating compaction and repair traffic]</p>"},{"location":"concepts/four-vs-of-nosql/#velocity-high-speed-data-processing","title":"Velocity: High-Speed Data Processing","text":"<p>Velocity encompasses both the speed at which data arrives in the system and the speed at which queries must be processed and responded to. High write-ingest or low-latency lookup workloads expose commit-path bottlenecks, hot-partition risks, and cache-invalidation challenges.</p> <p>In NoSQL database selection, velocity considerations often determine the fundamental architecture choices between different database types. The velocity challenge manifests in two primary dimensions: write velocity (the rate at which new data enters the system) and read velocity (the speed at which queries must be processed and results returned).</p>"},{"location":"concepts/four-vs-of-nosql/#ingestion-patterns-and-mitigation-techniques","title":"Ingestion Patterns and Mitigation Techniques","text":"Ingestion Pattern Mitigation Technique Sudden bursts (launch events) Auto-scaling write capacity or \"warm throughput\" Sustained firehose (IoT, click-streams) Streaming buffers (Kinesis/Kafka \u2192 NoSQL) Read-after-write immediacy Local-partition read routing, write-through cache Millisecond fan-out reads Adaptive RT caching, DAX / Redis fronts <p>NoSQL databases address velocity through various architectural approaches:</p> <p>Distributed Processing: Spreading both data and processing across multiple nodes to parallelize operations.</p> <p>Asynchronous Processing: Decoupling write operations from consistency checks to improve write throughput.</p> <p>Caching Strategies: Implementing multi-level caching to reduce query response times.</p> <p>Optimized Data Structures: Using specialized data structures like LSM trees or B+ trees optimized for specific access patterns.</p>"},{"location":"concepts/four-vs-of-nosql/#real-world-velocity-challenges","title":"Real-World Velocity Challenges","text":"<p>Disney+ Global Streaming: Disney+ ingests billions of viewer-interaction bookmarks per day through Kinesis streams into Amazon DynamoDB, then replays them at sub-50 ms latency so a user can resume playback on any device. AWS added a \"warm throughput\" pre-provisioning feature (January 2025) to keep SLAs during regional fail-overs. The velocity challenge involves handling spikes during popular content releases while maintaining consistent user experience globally.</p> <p>High-Frequency Trading: Financial trading platforms must process thousands of trades per second while maintaining microsecond-level latency for order matching. Systems like those used by major exchanges employ specialized NoSQL databases that can handle 100,000+ transactions per second while providing immediate consistency for account balances and position tracking.</p> <p>Real-Time Gaming: Multiplayer gaming platforms like those used by Epic Games for Fortnite must update player statistics and leaderboards in real-time across millions of concurrent players. The system must handle spikes of hundreds of thousands of score updates per second during peak gaming hours while providing immediate feedback to players.</p> <p>Social Media Live Events: During major events like the Super Bowl or World Cup, social media platforms experience extreme spikes in activity. Twitter has reported handling over 500,000 tweets per minute during peak moments, requiring NoSQL systems that can dynamically scale to handle these velocity spikes without degrading performance for regular users.</p> <p>Programmatic Advertising: Ad auction systems must process bid requests and responses in under 100 milliseconds while handling millions of requests per second. Companies like Google's AdX must evaluate multiple bid requests simultaneously, apply complex targeting rules, and return winning bids\u2014all within tight latency constraints that directly impact revenue.</p> <p>[Suggested Image: Timeline graphic of spikes in write throughput with annotations showing auto-scaling steps and latency targets]</p>"},{"location":"concepts/four-vs-of-nosql/#variability-handling-diverse-data-types","title":"Variability: Handling Diverse Data Types","text":"<p>Variability addresses the challenge of managing diverse data types, formats, and structures within a single system. When the schema itself changes frequently\u2014or each entity type adds fields at will\u2014rigid tables turn into friction points.</p> <p>Traditional relational databases require predefined schemas that specify exactly what data types and structures are permitted. NoSQL databases, however, are designed to handle schema flexibility and evolution, accommodating various forms of data variability.</p>"},{"location":"concepts/four-vs-of-nosql/#variability-drivers-and-nosql-responses","title":"Variability Drivers and NoSQL Responses","text":"Variability Driver NoSQL Feature Response Product catalog lines with unique attributes Document model, dynamic fields \"Polyglot\" event envelopes (e.g., sensor vs. log) Wide-column families, sparse rows Rapid A/B experiment metadata Flexible JSON sub-documents Long-tail attribute discovery Schema-on-read with search indexes <p>The variability challenge encompasses several dimensions:</p> <p>Schema Evolution: The ability to add new fields, modify existing structures, or change data types without requiring system downtime or complex migration procedures.</p> <p>Multi-Format Support: Handling structured, semi-structured, and unstructured data within the same system\u2014from JSON documents to binary files to graph relationships.</p> <p>Dynamic Schemas: Supporting data structures that can vary significantly between records, even within the same collection or table.</p> <p>Polymorphic Data: Managing objects that share some common properties but have significant structural differences.</p>"},{"location":"concepts/four-vs-of-nosql/#real-world-variability-challenges","title":"Real-World Variability Challenges","text":"<p>Fashion Retail Evolution: A leading fashion retailer migrated a highly variable product-catalog to MongoDB so every SKU can own bespoke attributes (color, fabric, bundle contents). Variable nesting forced them to re-index frequently; careless index explosions degraded write speed until they moved \"search-only\" facets into Atlas Search. The variability challenge involves supporting unlimited custom fields while maintaining query performance across diverse content types.</p> <p>Healthcare Data Integration: Hospital systems must integrate data from electronic health records, medical devices, imaging systems, and laboratory results. Each source provides data in different formats\u2014structured lab results in XML, unstructured physician notes in text, medical images in DICOM format, and device telemetry in JSON. NoSQL databases like MongoDB enable healthcare providers to store all this varied data while maintaining relationships between different data types for comprehensive patient records.</p> <p>WordPress.com Scale: WordPress.com hosts millions of websites, each with unique content structures, custom fields, and plugin data. Their NoSQL implementation must handle blog posts with standard fields (title, content, author) alongside highly customized data structures for e-commerce sites, portfolios, and corporate websites.</p> <p>Scientific Research Data: Genomics research generates highly variable data types\u2014DNA sequences, protein structures, experimental conditions, and analysis results. Research institutions use NoSQL databases to store everything from simple metadata records to complex nested structures representing molecular interactions. The challenge is maintaining data integrity and queryability across vastly different data structures while supporting rapid schema evolution as research methodologies advance.</p> <p>E-commerce Marketplaces: Online marketplaces like eBay must handle products ranging from simple items (books with ISBN, title, author) to complex configurable products (laptops with dozens of technical specifications) to services (consulting with time-based pricing). The variability challenge involves creating a flexible schema that can accommodate any product type while enabling efficient search and filtering across diverse attribute sets.</p> <p>[Suggested Image: Side-by-side depiction of a rigid relational ERD vs. a schemaless JSON document showing optional fields in grey]</p>"},{"location":"concepts/four-vs-of-nosql/#veracity-ensuring-data-quality-and-integrity","title":"Veracity: Ensuring Data Quality and Integrity","text":"<p>Veracity addresses the challenge of maintaining data quality, consistency, and trustworthiness in distributed NoSQL systems. At scale, silent corruption or bad upstream feeds quickly pollute downstream analytics and machine learning systems.</p> <p>Unlike traditional databases with strict ACID guarantees, NoSQL databases often trade consistency for availability and partition tolerance, making veracity a complex but crucial consideration.</p>"},{"location":"concepts/four-vs-of-nosql/#quality-concerns-and-nosql-techniques","title":"Quality Concerns and NoSQL Techniques","text":"Quality Concern NoSQL / Ecosystem Technique Late-arriving data or duplicates Idempotent upserts, dedup streams Schema drift &amp; null explosions Column-level quality rules, schema registry Corrupted batches Write-Audit-Publish (WAP) pattern with branch validation Governance &amp; lineage Metadata control plane (Purview, Atlan) <p>Veracity in NoSQL systems encompasses several critical dimensions:</p> <p>Data Consistency: Ensuring that all nodes in a distributed system have the same view of the data, often involving eventual consistency models rather than immediate consistency.</p> <p>Data Validation: Implementing rules and checks to ensure data meets quality standards, even in schema-flexible environments.</p> <p>Audit Trails: Maintaining records of data changes for compliance and debugging purposes.</p> <p>Conflict Resolution: Handling situations where concurrent updates create conflicting data states.</p> <p>Data Lineage: Tracking the origin and transformation history of data as it moves through the system.</p>"},{"location":"concepts/four-vs-of-nosql/#real-world-veracity-challenges","title":"Real-World Veracity Challenges","text":"<p>Apache Iceberg WAP Pattern: Cloud data lakes using Apache Iceberg implement WAP branches so each ingestion job writes to an isolation branch, runs AWS Glue Data Quality checks, and only merges into the main table on pass. Teams then surface lineage and rule failures through Atlan's catalog to root-cause faulty producers. This approach safeguards veracity at lakehouse scale by validating data in isolation before making it available to consumers.</p> <p>Financial Transaction Processing: Banks using NoSQL databases for transaction processing must ensure absolute accuracy while maintaining high availability. JPMorgan Chase's distributed systems must handle millions of transactions daily while ensuring that account balances remain consistent across all nodes. The veracity challenge involves implementing sophisticated consensus algorithms and validation rules that can detect and resolve conflicts without impacting transaction throughput.</p> <p>Supply Chain Traceability: Companies like Walmart use NoSQL databases to track products from manufacturers to stores, involving multiple data sources with varying reliability. The system must validate product information, track inventory levels, and maintain data integrity across suppliers, warehouses, and retail locations. Veracity challenges include handling conflicting inventory counts, validating supplier data accuracy, and ensuring traceability for food safety compliance.</p> <p>Healthcare Record Integrity: Healthcare providers using NoSQL databases for patient records must ensure data accuracy while supporting rapid access during emergencies. The veracity challenge involves validating medical data entry, maintaining consistency across different medical systems, and ensuring that critical patient information (allergies, medications, medical history) remains accurate and accessible. Any data quality issues could have life-threatening consequences.</p> <p>Customer Identity Management: Companies like Airbnb must maintain accurate customer profiles while integrating data from multiple sources\u2014social media accounts, payment systems, and verification services. The veracity challenge involves resolving conflicting information, validating identity documents, and maintaining data accuracy across different systems while protecting user privacy and preventing fraud.</p> <p>[Suggested Image: Flow chart of Write \u2192 Audit \u2192 Publish branches with red/yellow/green gates representing quality checks]</p>"},{"location":"concepts/four-vs-of-nosql/#balancing-the-four-vs-in-database-selection","title":"Balancing the Four V's in Database Selection","text":"<p>The four V's rarely exist in isolation\u2014most real-world applications must address multiple dimensions simultaneously. Understanding how different NoSQL database types handle these challenges helps inform architectural decisions:</p> <p>Document Databases (MongoDB, CouchDB) excel at handling variability and moderate volume, making them ideal for content management and rapid application development.</p> <p>Key-Value Stores (Redis, DynamoDB) optimize for velocity and volume, perfect for caching and session management.</p> <p>Column-Family Databases (Cassandra, HBase) handle volume and velocity exceptionally well, making them suitable for time-series data and analytics.</p> <p>Graph Databases (Neo4j, Amazon Neptune) specialize in complex relationships while maintaining veracity, ideal for social networks and recommendation systems.</p>"},{"location":"concepts/four-vs-of-nosql/#quick-reference-trade-offs","title":"Quick-Reference Trade-offs","text":"V Primary Risk if Ignored Typical Mitigation Volume Unbounded storage costs, repair lag Tiered storage, cold/offline compaction Velocity Hot partitions, timeout errors Auto-scaling, adaptive partition keys Variability Rigid schema migrations, code debt Document/column family, schema-on-read Veracity Bad decisions from bad data WAP, data-quality rules, lineage tools"},{"location":"concepts/four-vs-of-nosql/#integrating-the-four-vs-in-atam-trade-off-analysis","title":"Integrating the Four V's in ATAM Trade-off Analysis","text":"<p>By systematically evaluating Volume, Velocity, Variability, and Veracity through ATAM, architects can justify database choices that stand the test of scale\u2014rather than discovering painful limits in production.</p> <ol> <li> <p>Elicit utility scenarios that explicitly reference each V (e.g., \"ingest 10 GB/s from 1 M IoT devices with &lt; 100 ms eventual query latency\").</p> </li> <li> <p>Map design tactics (partitioning, compaction, WAP, etc.) to those scenarios and rate their impact on other qualities such as cost, availability, and maintainability.</p> </li> <li> <p>Identify sensitivity points\u2014places where a small change in any V (e.g., velocity spike during Black Friday) forces disproportionate architectural cost.</p> </li> <li> <p>Document trade-off implications so stakeholders understand why, for example, a high-velocity system might accept eventual consistency to keep write SLAs.</p> </li> </ol> <p>The key to successful NoSQL database selection lies in understanding which V's are most critical for your specific use case and choosing technologies that align with those priorities while providing acceptable performance in other dimensions.</p> <p>[Suggested Image: A matrix showing different NoSQL database types rated against each of the four V's, helping visualize the trade-offs between different technologies]</p>"},{"location":"concepts/four-vs-of-nosql/#summary","title":"Summary","text":"<p>The four V's of NoSQL\u2014Volume, Velocity, Variability, and Veracity\u2014provide a comprehensive framework for evaluating database technologies in the context of scalable system design. Each dimension presents unique challenges that must be carefully considered during the architectural decision-making process.</p> <p>Volume challenges require sophisticated distributed storage strategies and operational excellence in managing large-scale data operations. Velocity demands high-performance architectures that can handle both write-intensive and read-intensive workloads with minimal latency. Variability necessitates flexible schemas and data models that can evolve with changing business requirements. Veracity requires robust data quality mechanisms and governance frameworks to ensure trustworthy data at scale.</p> <p>By understanding these dimensions and their real-world implications, architects can make informed decisions that balance immediate requirements with long-term scalability needs. The ATAM methodology provides a structured approach to evaluating these trade-offs, ensuring that database selection decisions are grounded in explicit quality attribute scenarios rather than technology preferences.</p> <p>Success in NoSQL database selection comes not from optimizing for all four V's simultaneously, but from understanding which dimensions are most critical for your specific use case and choosing technologies that excel in those areas while providing acceptable performance in others.</p>"},{"location":"concepts/four-vs-of-nosql/#references","title":"References","text":"<ol> <li> <p>Facebook's Database Handling Billions of Messages (Apache Cassandra\u00ae Deep Dive) - Mar 11 2025 - ByteByteGo Newsletter - Case study of Apache Cassandra powering Facebook Messenger, highlighting petabyte-scale Volume and the operational strain of compaction &amp; repairs.</p> </li> <li> <p>Amazon DynamoDB introduces warm throughput for tables and indexes in the AWS GovCloud (US) Regions - Jan 22 2025 - AWS What's New - Announces \"warm throughput\" pre-provisioning to cushion sudden write Velocity spikes.</p> </li> <li> <p>Amazon DynamoDB use cases for media and entertainment customers - Jun 26 2024 - AWS Database Blog - Details how Disney+ stores watch-position bookmarks in global tables for sub-50 ms read/write latency, exemplifying high-Velocity workloads.</p> </li> <li> <p>Building with Patterns: The Attribute Pattern - Feb 13 2019 - MongoDB Blog - Explains index explosion caused by highly variable product-catalog attributes, illustrating Variability performance trade-offs.</p> </li> <li> <p>Build Write-Audit-Publish pattern with Apache Iceberg branching and AWS Glue Data Quality - Dec 09 2024 - AWS Big Data Blog - Shows how the WAP pattern validates data in isolation branches before merge, safeguarding Veracity at lakehouse scale.</p> </li> <li> <p>Apache Iceberg Architecture: 3 Core Components to Understand - Apr 2025 - Atlan Blog - Describes Atlan's Polaris\u2013based integration that surfaces lineage and quality metadata for Iceberg tables, strengthening Veracity governance.</p> </li> </ol>"},{"location":"concepts/utility-tree/","title":"Utility Tree","text":""},{"location":"concepts/utility-tree/#overview","title":"Overview","text":"<p>A Utility Tree, also known as a Quality Tree, is a hierarchical model that represents various quality attributes (often non-functional requirements) that are significant for the success of a software project. It helps stakeholders to prioritize requirements by assessing their importance and the ease with which they can be fulfilled by a given architecture.</p>"},{"location":"concepts/utility-tree/#creating-an-intuitive-measure-of-fitness-for-a-task","title":"Creating an Intuitive Measure of Fitness for a Task","text":"<p>Our goal in creating quality tree diagrams is to create an intuitive visualization of the \"fitness\" of a database for a given application. Think of this as trying to see if a glove fits your hand.  Each finger needs to fit well in the glove.</p> <p>Think of each finger as a \"dimension\" of fitness.  At the end of trying on gloves, you will get an overall feeling of how specific gloves fit.  Our goal is to not focus on just one dimension of fitness, but to get a more holistic feeling for the fitness of the most critical aspects of the suitability of a database for a project.</p>"},{"location":"concepts/utility-tree/#non-functional-requirements-nfrs","title":"Non-Functional Requirements (NFRs)","text":"<p>Non-functional requirements are criteria that specify the operation of a system, as opposed to the behaviors or functions the system must perform. These include aspects like scalability, performance, security, and usability. In the context of databases, NFRs are crucial because they define how the system should behave under various conditions and constraints.</p> <p>Examples of Non-Functional Requirements for a Database:</p> <ol> <li>Scalability: The ability of the database to handle increased loads by adding resources.</li> <li>Availability: The degree to which the database is operational and accessible when required for use.</li> <li>Security: Protection of data against unauthorized access and ensuring confidentiality, integrity, and availability of data.</li> <li>Performance: The speed with which the database processes transactions and returns results.</li> <li>Maintainability: How easily the database can be modified to add new features, fix bugs, or improve performance.</li> </ol> <p>Quality Tree for Database Selection: The image provided exemplifies a Quality Tree where each \"ility\" is a branch representing a key quality attribute of a database. Each attribute is further broken down into specific characteristics that can be evaluated.</p> <p>The \"-ilities\" Listed with Two Scores: Each quality attribute is assessed based on two dimensions:</p> <ol> <li>Importance to the success of the project (I): Ranks how critical the attribute is to the project's success, scored as Critical (C), High (H), Medium (M), or Low (L).</li> <li>Ease of fulfillment by the architecture (E): Measures how easily a given architecture can fulfill the requirement, scored as Easy (E), Medium (M), or Hard (H).</li> </ol> <p>Quality Tree Descriptions Based on the Image:</p> <ol> <li> <p>Scalability (H, L): Critical for handling growth but can be challenging to implement, requiring the architecture to manage data across multiple nodes and support distributed queries.</p> </li> <li> <p>Availability (H, L): High importance for continuous operation, particularly in distributed systems where fault tolerance and automatic data migration to new nodes are essential.</p> </li> <li> <p>Findability (H, L): The ease of locating information via full-text search and customizable ranking is crucial for user satisfaction.</p> </li> <li> <p>Schemaless (H, L): Important for flexibility in handling various data types without predefined models, which can be both a boon and a challenge depending on the use case.</p> </li> <li> <p>Queryability (H, L): A database's capacity to query any data attribute and fit the query language to the problem space is essential for effective data retrieval.</p> </li> <li> <p>Transformability (H, M): The ability to easily write data transformations and distribute them over many servers is critical for data processing but might require moderate effort to implement.</p> </li> <li> <p>Affordability (H, L): Cost is always a consideration; open-source licenses offer significant savings but may vary in ease of integration with existing systems.</p> </li> <li> <p>Interoperability (M, H): The ability to integrate with reporting tools and standardized systems is highly important for a holistic data environment, often facilitated by adherence to standards.</p> </li> <li> <p>Security (H, H): Ensuring robust security measures like role-based access control and audit capabilities is critical and typically requires substantial effort to implement effectively.</p> </li> </ol> <p>In summary, a Quality Tree is a visual representation that helps in the decision-making process when selecting a database for a project. It lays out the NFRs in a structured format, allowing stakeholders to discuss, prioritize, and decide which qualities are most important and how feasible they are to implement with the chosen architecture. This approach enables a balanced assessment of potential trade-offs and ensures that the selected database aligns with the project's goals and constraints.</p>"},{"location":"db-types/","title":"Database Architecture Types","text":"<p>In this section, we will cover the six key database architecture types we use when selecting the right database architecutre.</p> <p></p> <p>The six types are:</p> <ol> <li>Relational</li> <li>Analytical (OLAP Cubes)</li> <li>Key Value Stores</li> <li>Column Family Stores</li> <li>Graph</li> <li>Document</li> </ol> <p>Each of these database architectures have many sub-types and each of these have pros and cons for different applications.</p> <p>It today's AI-driven world, graphs have become a major force. We will cover this topic in the next section.</p>"},{"location":"db-types/key-value/","title":"Key Value Stores","text":""},{"location":"db-types/key-value/#quotes","title":"Quotes","text":"<p>Simplify, simplify, simplify. Henry David Thoreau</p> <p>Simplicity is the ultimate sophistication. Leonardo da Vinci</p> <p>Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away. \u2014Antoine de Saint-Exup\u00e9ry, author of The Little Prince</p> <p>Any intelligent fool can make things bigger, more complex and more violent. It takes a touch of genius and a lot of courage to move in the opposite direction.  \u2014Albert Einstein</p>"},{"location":"db-types/key-value/#_1","title":"Key Value","text":""}]}